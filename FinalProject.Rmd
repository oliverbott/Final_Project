---
title: 'Open vs. closed innovation: using online network data to measure innovation'
author: "Benjamin Snow and Oliver Bott"
date: "14 November 2014"
linestretch: 2
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: true
    toc_depth: 2
  html_document:
    fig_caption: yes
    number_sections: no
bibliography:
- Packages.bib
- Main.bib
---

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
# Set working directory. Change as needed
setwd('C:/Users/a6p/Desktop/Uni 2014/E1161 - Collaborative Research/GitHub Clone/Final_Project/')

require("devtools")
library(RCurl)
library(ggmap)
library(maptools)
library(dplyr)
library(reshape2)
library(maps)
library(stargazer)
library(rCharts)

# Load packages and create BibTeX file
# Note: must have repmis, rsdmx, httr, dplyr and rjson packages installed
PackagesUsed <- c("ggplot2", "repmis", "rsdmx", "httr", 
                  "dplyr", "rjson", "stargazer", "knitr", 
                  "car", "rCharts", "reshape2", "ggmap", 
                  "maps", "RCurl", "maptools", "Rcpp")

# Load PackagesUsed and create .bib BibTeX file
repmis::LoadandCite(PackagesUsed, file = "Packages.bib", install = FALSE)

```

# Background

This research project examines the potential benefit of using open knowledge data in the form of collaborative online network data as an innovation indicator. By doing so, this work critically assesses current innovation indicators, namely patent data, in the hope of offering new alternatives for measuring and understanding innovation. The stated research question is:  *To what extent can open innovation network data add to the measurement of innovation performance?*

For the full examination of the previous literature on the subject and reasoning for the purpose, motives, and plan for this study please see the [Research Proposal](https://github.com/benjaminsnow/Collaborative-Research-Proposal-Assignment-Two-).  This contribution focuses specifically on outlining the data gathering and cleaning process. It will also examine the constructed dataset using basic descriptive statistics, as well as run preliminary inferential statistical models, offering explanation and context throughout the process. Finally, steps to improve the analysis for the final version will be discussed.

# Methodology

## Data Gathering

To examine open network data against patent data, this study relies on two key data sources and uses the statistical tool *R* [@CiteR] for the data analysis.

The first data set is obtained by using the Application Programming Interface (API) data for open networks. To examine open innovation, data is obtained from the the git repository web-based hosting service GitHub[^GIT]. The *R* [@CiteR] packages *httr* [@R-httr], *dplyr* [@R-dplyr] and *rjson* [@R-rjson] allow for compiling data on the follower counts and locations associated with different users and online reponsitories. This analysis examines three follower categories (users with x followers per 10,000 population). The variable of no followers is a general indicator of GitHub use in a given location. We also include users with a follower range of 1-24 as an indicator for medium intensity of collaboration. The third category includes GitHub users with more than 25 followers, which acts as an indicator of high collaboration and innovative activity.

As an indicator of closed innovation, city-level patent registration data is used from the Organization for Economic Co-operation and Development[^OECD]. This study uses Patent Cooperation Treaty (PCT) patent applications per 10,000 inhabitants on the city-level. From the same database, GDP, employment and environmental data are used as additional variables which could prove significant in explaining differences in innovation. The *R* [@CiteR] package *rsdmx* [@R-rsdmx] is necessary for obtaining the OECD dataset.

All data obtained via the GitHub API and OECD database can be linked to individual cities (n=120) in a total of 15 countries, allowing for an analysis on the regional level. The code used for gathering and cleaning the data is stored in a separate .R file and can be accessed [here](https://github.com/oliverbott/Final_Project/blob/master/Data/Data_Gathering.R).


```{r, include=FALSE, warning=FALSE, error=FALSE, message=FALSE}
# Load Dataset.csv from repository
library(RCurl)

# Set SSL certs globally
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))

# Create dynamic link to Dataset.csv created through the Data_Gathering.R
x <- getURL('https://raw.githubusercontent.com/oliverbott/Final_Project/master/Data/Dataset.csv')
dataset <- read.csv(text = x)

# Clean dataset
dataset$row.names <- NULL
dataset$X <- NULL

# Change column names
colnames(dataset) <- c("METRO_ID" , "Patents" , "GDP" , "Population", "Greenspace", "Employment" , "Pollution" , "lon" , "lat" , "AnyFollowing" , "1" , "10" , "20" , "30" , "40" , "50" , "60" , "70" , "80" , "90" , "100" , "US" , "DE" , "FR" , "JP" , "Users")
```

## Data Sources

As can be seen in the Table below, the analysis is based on cross-sectional data with varying time frames. There are several limitations to the data used in this study. First, the time discrepency between different aspects of the data used, which range from 2005 to 2014, reflect an obvious data comparability constraint. Secondly, in this analysis for data availability and access reasons, there are some prominent innovation hubs excluded, including San Fransisco and New York. Third, several variables used, with pollution being the most obvious example, while numerically accurate, act as at best a rough proxy for the meaning (industrialization) attributed to them. Any found significance will need to take this into account. Last, and perhaps most significantly, when comparing the two measures of innovation, it should be noted that patent data reflects innovation across all types of sectors, whereas Github data mainly reflects innovation within the software technology domain.

                         Table 1: Data sources and explanations.

```{r, echo=FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE}
library(knitr)

# Create table with info on variables
Variables <- c('Patents', 'GDP', 'Population', 'Greenspace', 
               'Employment', 'Pollution', 'No Following', 
               '1-24 Following', '>25 Following')
Explanation <- c('PCT patents per 10,000 population', 'GDP per capita', 
                 'Total urban population', 'Green area per capita in square metres', 
                 'Employment of metropolitan area as % of national value', 
                 'Annual average of pop exposure to air pollution PM2,5 in µg/m³', 
                 'GitHub users per 10,000 population with x followers', 
                 'GitHub users per 10,000 population with x followers', 
                 'GitHub users per 10,000 population with x followers')
Year <- c('2008', '2008','2008','2008','2008',
          '2005', '2014', '2014', '2014')
Source <- c('OECD', 'OECD', 'OECD', 'OECD', 'OECD', 
            'OECD', 'GitHub', 'GitHub','GitHub')


# Combine
limtab <- data.frame(Variables, Explanation, Year, Source)

knitr::kable(limtab)

```


## Data Selection

Several potential explanatory variables are collected besides the patent and GitHub data. These variables were selected as they were thought to potentially show cause for why innovation, be it open or closed, occurs in a certain city, but needed to be variables that would not introduce endogeneity to the model. 

**Greenspace**: The Greenspace indicator is deemed potentially relevant in that with a choice of city to innovate in (assuming some level of geographic labor flexibility) there might be a recreational value necessary for attracting talent. Put another way, green cities could attract innovators.

**Pollution**: The Pollution indicator is taken both as a broad proxy for industrialization (leaving aside a discussion of to what degree pollution is from industry vs cars), that it seemed worth exploring whether a certain level of pollution discouraged talent attraction of innovators on the city-level.

**Employment**: The Employment indicator is taken largely as an indication of that city´s significance within its national context. Understanding whether a city would likely be viewed as the most prominent or significant, and whether this effects innovation, or whether innovation takes place in smaller provincial cities, is worther understanding.  Additionally, seeing if the type of innovation (open vs. closed) depends on the significance of the city is viewed as relevant.

**GDP**: A GDP indicator explores whether the size of the economy, or wealth generally, is related to innovation on the city-level, and if it is indicator of one type of innovation over another.

**Population**: A Population variable explores whether there is a necessary city size threshold which corresponds to innovation, and also is taken for controlling for across cities, to find patent data or GitHub data per a number of people in a city.

Follower cutoff point

```{r Follower Cutoff Selection, echo=FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE}
# Load .csv from GitHub repository
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))

# Create dynamic link to Dataset.csv created through the Data_Gathering.R
x <- getURL('https://raw.githubusercontent.com/oliverbott/Final_Project/9c08cb04289524812f7f2a94a00bd5d89b7fa7de/Data/Dataset.csv')
datasetold <- read.csv(text = x)

# For this task we get rid of the other variables
datasetold$X <- NULL
datasetold$Patents <- NULL
datasetold$GDP <- NULL
datasetold$Population <- NULL
datasetold$Pollution <- NULL
datasetold$Greenspace <- NULL
datasetold$Employment <- NULL
datasetold$lon <- NULL
datasetold$lat <- NULL
datasetold$zeroFollowers <- NULL

new <- datasetold
new_2 <- reshape(new, 
                 varying = c("US", "DE", "FR", "JP"), 
                 v.names = "value",
                 timevar = "Country", 
                 times = c("US", "DE", "FR", "JP"), 
                 new.row.names = 1:1000,
                 direction = "long")

new_2 <- subset(new_2, value>0)
new_2$id <- NULL 
new_2$value <- NULL
new_2$Country <- as.factor(new_2$Country)
x_2 <- split(new_2, new_2$Country)
x_3 <- lapply(x_2, function (x) sapply(x, mean))
x_3 <- data.frame(x_3)
x_3 <- na.omit(x_3)

df <- reshape(x_3, 
                 varying = c("US", "DE", "FR", "JP"), 
                 v.names = "value",
                 timevar = "Country", 
                 times = c("US", "DE", "FR", "JP"), 
                 new.row.names = 1:1000,
                 direction = "long")
variable <- c(1,10,20,30,40,50,60,70,80,90,100, 
              1,10,20,30,40,50,60,70,80,90,100,
              1,10,20,30,40,50,60,70,80,90,100,
              1,10,20,30,40,50,60,70,80,90,100)
df <- data.frame(df, variable)
df$id <- NULL 
df$variable <- as.factor(df$variable)

# Set color scheme
library(RColorBrewer)
myColours <- brewer.pal(6,"Set3")

my.settings <- list(
  superpose.polygon=list(col=myColours[3:6], border="transparent"),
  strip.background=list(col=myColours[6]),
  strip.border=list(col="black")
)

require(lattice)
barchart(value ~ variable, groups=Country, df, auto.key = list(columns = 4), 
         main="Follower counts in different countries", xlab="Follower category", 
         ylab="Frequency", par.settings = my.settings, scales=list(alternating=1),
         uto.key=list(space="top", columns=4, 
                      points=FALSE, rectangles=TRUE,
                      title="District", cex.title=1),
         par.strip.text=list(col="white", font=2),)

```

# Analysis

## Descriptive Statistics

The summary statistics in Table 2 show wide ranging distributions of the observations in the data set. Since the data cleaning eliminated all values equal to or lower than zero, a log transformation seems to be a strategy that could strengthen the analysis. The *car* package [@R-car] is used to examine the relationship, distribution, and normality of all variables included in the model, to understand which regression model would be most appropriate. The distribution of many variables are highly skewed (see Figure 1). All of the GitHub based variables 'nofollowing', 'medfollowing', and 'hifollowing' have significant right skews, as do nearly all of the observed variables, excluding Pollution, GDP and Patents, which come closer to a normal distribution. It seems as if already in the scatterplot a slight correlation between Patents and Followers can be observed. To normalize for the skewed distributions, the log of the variables is deemed necessary to increase the explanatory power of our inferential statistics.  


```{r Summary Statistics of selected variables, echo=FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE}
# Descriptive statistics
library(stargazer)

sumdat <- subset(dataset, select = -c(1,8:9,22:25) )

colnames(sumdat) <- c("Patents" , "GDP" , "Population", "Greenspace", "Employment" , "Pollution" , "Users with >0 Followers" , "1-9 Followers" , "10-19 Followers" , "20-29 Followers" , "30-39 Followers" , "40-49 Followers" , "50-59 Followers" , "60-69 Followers" , "70-79 Followers" , "80-89 Followers" , "90-99 Followers" , ">100 Followers" , "Users with >20 Followers")

stargazer(sumdat[,], summary=TRUE,
  title = 'Summary statistics',
  digits = 2, type ='latex', header=FALSE)

```


```{r Scatterplot matrix of selected variables, echo=FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE}
# Create scatterplot matrix of variables
require("car")

scatmat <- subset(dataset, select = -c(1,8:9,11:25) )

car::scatterplotMatrix(scatmat[,], 
                       main="Scatterplot matrix")

```

The residual plot between patents and users with high follower numbers (see Figure 2) depicts a relatively random pattern, which indicates that a linear regression model provides a decent fit to the inferential statistics of the data set.

```{r, echo=FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE}

# For log delete 0 follower categories: new datasetlog
datasetlog <- dataset
datasetlog <- datasetlog[-c(45, 59, 62, 63, 73, 81, 90, 116, 121), ]

datasetlog2 <- dataset
datasetlog2 <- datasetlog2[-c(45, 62, 90), ]

# Create residual plot for log(Patents)
patfol.lm = lm(log(Patents) ~ log(Users)
               , data = datasetlog)
patfol.res = resid(patfol.lm)

plot(datasetlog$Users, patfol.res,
     ylab="Residuals", xlab="High Followers",
     main="Patents")
abline(0, 0)

```


```{r Regression Preparation, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
# Regression table with stargazer

# Create country interaction
datasetlog$USfol <- datasetlog$Users*datasetlog$US
datasetlog$DEfol <- datasetlog$Users*datasetlog$DE
datasetlog$FRfol <- datasetlog$Users*datasetlog$FR
datasetlog$JPfol <- datasetlog$Users*datasetlog$JP

# Create regression models with Patents as dependent variable
P1 <- lm(log(Patents) ~ log(Users), data = datasetlog)
P2 <- lm(log(Patents) ~ log(Users) + log(GDP) + log(Population) + 
           log(Greenspace) + log(Employment) + log(Pollution), data = datasetlog)
P3 <- lm(log(Patents) ~ log(Users) + log(GDP) + log(Population) + 
           log(Greenspace) + log(Employment) + log(Pollution), data = datasetlog)

labelsP <- c('Over 20 Followers', 'GDP', 'Population', 'Greenspace', 'Employment', 'Pollution', '(Intercept)')

# Create regression models with User numbers as dependent variable
F1 <- lm(log(Users) ~ log(Patents), data = datasetlog)
F2 <- lm(log(Users) ~ log(Patents) + log(GDP) + log(Population) + 
           log(Greenspace) + log(Employment) + log(Pollution), data = datasetlog)
F3 <- lm(log(Users) ~ log(Patents) + log(GDP) + log(Population) + 
           log(Greenspace) + log(Employment) + log(Pollution) + US + DE + FR + JP, data = datasetlog)

labelsF <- c('Patents', 'GDP', 'Population', 'Greenspace', 'Employment', 'Pollution', 'US' , 'DE' , 'FR' , 'JP', '(Intercept)')

# Create regression models with any Following numbers as dependent variable
D1 <- lm(log(AnyFollowing) ~ log(Patents), data = datasetlog2)
D2 <- lm(log(AnyFollowing) ~ log(Patents) + log(GDP) + log(Population) + log(Greenspace) + log(Employment) + log(Pollution), data = datasetlog2)
D3 <- lm(log(AnyFollowing) ~ log(Patents) + log(GDP) + log(Population) + log(Greenspace) + log(Employment) + log(Pollution) + US + DE + FR + JP, data = datasetlog2)

labelsD <- c('Users', 'GDP', 'Population', 'Greenspace', 'Employment', 'Pollution', 'US' , 'DE' , 'FR' , 'JP', '(Intercept)')
```

## Inferential Statistics

As a relationship between patent and follower data is observed, further inferential statistical analysis attempts to find the common predictor or cause of innovation in both patent and open data. The second regression model hence includes the open innovation indicator now as the dependent variable *F* and is expressed below using similar notation and logic as stated above:

$$   logF_{l} = {\beta}_0 +{\beta}_1 logP_{l} + {\beta}_2 logGDP_{l} + {\beta}_3 logPop_{l} + {\beta}_4 logG_{l} + {\beta}_4 logE_{l} + {\beta}_4 logPol_{l} + {\epsilon}_{l}$$

```{r, echo=FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE}
stargazer::stargazer(D1, D2, D3, covariate.labels = labelsD,
  title = 'Regression Estimates of GitHub Collaborative Activity',
  digits = 2, type ='latex', single.row=TRUE)
```


As can be seen in Table 4, again Patents are strongly positively correlated with Follower numbers (at a significance level of p<0.01). Both Pollution and Employment are negatively correlated (at a significance level of p<0.05). The adjusted R squared value indicates that about 36% of the variation in follower numbers in our sample is explained through the model.

```{r, echo=FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE}
stargazer::stargazer(P1, P2, P3, covariate.labels = labelsP,
  title = 'Regression Estimates of Patent Activity',
  digits = 2, type ='latex', single.row=TRUE)
```


This study plans to use an ordinary least squares (OLS) model to examine the relationship between patent and highly followed open data sources. The model for the regression analysis can be viewed as:

$$   logP_{i} = {\beta}_0 +{\beta}_1 logF_{i} + {\beta}_2 logGDP_{i} + {\beta}_3 logPop_{i} + {\beta}_4 logG_{i} + {\beta}_5 logE_{i} + {\beta}_6 logPol_{i} + {\epsilon}_{i}$$

```{r, echo=FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE}
stargazer::stargazer(F1, F2, F3, covariate.labels = labelsF,
  title = 'Regression Estimates of GitHub Collaborative Activity',
  digits = 2, type ='latex', single.row=TRUE)
```


Here *P* is the patent intensity expected in a given city *i*. As seen in the regression output Table 3, a positive relationship between patent data and GitHub data is observed (at a significance level of p<0.01), though most significantly between patent data and those with high numbers of followers on GitHub. In the full model specification, a 1 percent increase in GitHub users with more than 25 followers (per 10,000 population) corresponds with a 1.76 percent increase in PCT patents (per 10,000 population). Additionally, Employment seems also to be positively correlated with patents, supporting the initial hypothesis that the significance of a city in a national context is strongly related to patent activities. Pollution is negatively correlated with patent data at a high significance level, while the variables GDP, Population and Greenspace do not seem to have a significant effect on patent activity.


The findings endorse the implicit hypothesis of the study that open data sources seem to show innovation in a similar but perhaps distinct manner to patent data and could hence enrich the measurement of innovation. These found relationships offer a glimps into the 'throughput' of open innovation (in the form of collaboration) rather than the 'output' which patent data reflects (in the form of commercialization of knowledge).  While significant relationships are found with innovation, other unaccounted for variables can be expected to contribute but are not currently accounted for in the model.

# Discussion


# Outlook

From this analysis it becomes apparent that the introduction of various dummy controls could help explain the spurious relationsbetween patent and network data. This could also help to find the more fundamental factors influencing innovation activity. Hence, to better answer the stated research question, it seems sensible to control for English speaking countries, as one would suspect the spread of GitHub to be greatest there. Also, one could introduce dummy controls for the overall economic development of the country, assuming that software development is clustered in these locations. In addition, including a map visualization with information on location of the cities in the sample could improve this work.

[^OECD]: Online accessible via [http://stats.oecd.org/](http://stats.oecd.org).
[^GIT]: Online accessilbe via [https://github.com/](https://github.com/).

# References
